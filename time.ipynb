{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s934F1fF23ja",
        "outputId": "92d4eae6-15e0-4773-e070-e016d98390f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-fe302d509ea6>:6: DeprecationWarning: Please use `center_of_mass` from the `scipy.ndimage` namespace, the `scipy.ndimage.measurements` namespace is deprecated.\n",
            "  from scipy.ndimage.measurements import center_of_mass\n"
          ]
        }
      ],
      "source": [
        "import torch, numpy as np, pandas as pd\n",
        "import sklearn as sk\n",
        "from datetime import datetime\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from scipy.ndimage.measurements import center_of_mass\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir attempt1"
      ],
      "metadata": {
        "id": "ZfgahOuwPWw0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCGrvGPS4w3e"
      },
      "source": [
        "## Data Preparation ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-A7dqvyZ2581"
      },
      "outputs": [],
      "source": [
        "INPUT_PATH = 'https://cernbox.cern.ch/remote.php/dav/public-files/dbwReGGBIPARvqM/converted_all.csv'\n",
        "layers = [0]\n",
        "cell_sizes_cm = [1.515, 3.03, 4.04, 6.06, 12.12]\n",
        "#cell_sizes_cm = [1.515]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CxO3i_Ju3EcT"
      },
      "outputs": [],
      "source": [
        "def get_time_cell_3x3(row):\n",
        "    time = row[:9]\n",
        "    energy = row[9:]\n",
        "    return (time * energy).sum() / (energy.sum() + 1e-6)\n",
        "\n",
        "\n",
        "def get_time_cell_5x5(row):\n",
        "    time = row[:25]\n",
        "    energy = row[25:]\n",
        "    return (time * energy).sum() / (energy.sum() + 1e-6)\n",
        "\n",
        "\n",
        "def get_features(df):\n",
        "    df['xGen_pred'] = 0.0\n",
        "    df['yGen_pred'] = 0.0\n",
        "    df['p_pred'] = 0.0\n",
        "    df['t_pred'] = 0.0\n",
        "\n",
        "    for layer in layers:\n",
        "        df['l{}_sum_3x3'.format(layer)] = \\\n",
        "            df[['l{}_{}'.format(layer, i) for i in [6,7,8,\n",
        "                                                    11,12,13,\n",
        "                                                    16,17,18]]].values.sum(axis=1)\n",
        "\n",
        "        df['l{}_sum_5x5'.format(layer)] = \\\n",
        "            df[['l{}_{}'.format(layer, i) for i in range(25)]].values.sum(axis=1)\n",
        "\n",
        "\n",
        "    df['z_ref'] = np.zeros_like(df['entry_x'].values)\n",
        "    df.loc[df['cell_size'] == 1.515, 'z_ref'] = 7.0\n",
        "    df.loc[df['cell_size'] == 3.03, 'z_ref'] = 10.0\n",
        "    df.loc[df['cell_size'] == 4.04, 'z_ref'] = 17.0\n",
        "    df.loc[df['cell_size'] == 6.06, 'z_ref'] = 17.0\n",
        "    df.loc[df['cell_size'] == 12.12, 'z_ref'] = 0.0\n",
        "    \n",
        "    barycenter = np.apply_along_axis(get_center_cluster, axis=1,\n",
        "                                     arr = df[['cell_size'] + \\\n",
        "                                              ['l{}_{}'.format(0, i) for i in range(25)]].values)       \n",
        "        \n",
        "    for i, axis in enumerate(['x','y']):\n",
        "        df['l0_{}_bar'.format(axis)] = \\\n",
        "            barycenter[:,i] + \\\n",
        "            df['{}_seed_shift'.format(axis)].values * df['cell_size'].values\n",
        "\n",
        "        df['entry_{}_new'.format(axis)] = \\\n",
        "            (df['entry_{}'.format(axis)].values*0.1 + \\\n",
        "             df['z_ref'].values * (df['p{}'.format(axis)].values / df['pz'].values)) * 10\n",
        "\n",
        "### angle\n",
        "        df['l0_{}_bar_cal'.format(axis)] = \\\n",
        "            np.floor(df['entry_{}'.format(axis)].values * 0.1 / df['cell_size'].values) * \\\n",
        "            df['cell_size'].values + 0.5 * df['cell_size'].values + \\\n",
        "            df['l0_{}_bar'.format(axis)]\n",
        "\n",
        "        df['l0_{}Rec_loc'.format(axis)] = \\\n",
        "            df['l0_{}_bar_cal'.format(axis)].values - \\\n",
        "            (np.floor(df['l0_{}_bar_cal'.format(axis)].values / df['cell_size'].values) * \\\n",
        "             df['cell_size'].values + 0.5 * df['cell_size'].values)\n",
        "\n",
        "        df['l0_{}Gen_loc'.format(axis)] = \\\n",
        "            df['entry_{}_new'.format(axis)].values*0.1 - \\\n",
        "            (np.floor(df['l0_{}_bar_cal'.format(axis)].values / df['cell_size'].values) * \\\n",
        "             df['cell_size'].values + 0.5 * df['cell_size'].values)\n",
        "    \n",
        "    for layer in layers:\n",
        "        df['t{}_weighted_3x3'.format(layer)] = np.apply_along_axis(get_time_cell_3x3, \n",
        "                                                    axis=1, \n",
        "                                                    arr=df[['t{}_{}'.format(layer,c) for c in [6,7,8,11,12,13,16,17,18]] + \\\n",
        "                                                           ['l{}_{}'.format(layer,c) for c in [6,7,8,11,12,13,16,17,18]]].values)\n",
        "\n",
        "        df['t{}_weighted_5x5'.format(layer)] = np.apply_along_axis(get_time_cell_5x5, \n",
        "                                                    axis=1, \n",
        "                                                    arr=df[['t{}_{}'.format(layer,c) for c in range(25)] + \\\n",
        "                                                           ['l{}_{}'.format(layer,c) for c in range(25)]].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pn0GZDRM3_CM"
      },
      "outputs": [],
      "source": [
        "def get_center_cluster(row):\n",
        "    return (np.array(center_of_mass(row[1:].reshape(5,5))) - 2.0) * row[0]\n",
        "\n",
        "\n",
        "def get_rmse(x, y):\n",
        "    return np.nanmean(((x - y) ** 2.0)) ** 0.5\n",
        "\n",
        "\n",
        "def get_rmse_metric(x, y, folds=5):\n",
        "    if x.shape != y.shape:\n",
        "        print('x.shape != y.shape')\n",
        "        raise\n",
        "\n",
        "    splits = np.array_split(np.arange(len(x)), folds)\n",
        "\n",
        "    rmse = []\n",
        "    for split in splits:\n",
        "        rmse.append(get_rmse(x[split], y[split]))\n",
        "\n",
        "    return np.nanmean(rmse), np.nanstd(rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mhMq40Ml3Vpi"
      },
      "outputs": [],
      "source": [
        "def calib(df):\n",
        "    cell_sizes = [1.515, 3.03, 4.04, 6.06, 12.12]\n",
        "\n",
        "    A = dict()\n",
        "    B = dict()\n",
        "\n",
        "    A[0] = dict()\n",
        "    B[0] = dict()\n",
        "    A[0][1.515] = 0.00497\n",
        "    B[0][1.515] = 109.5\n",
        "    A[0][3.03] = 0.09641\n",
        "    B[0][3.03] = 23.4\n",
        "    A[0][4.04] = 0.2382\n",
        "    B[0][4.04] = 70.72\n",
        "    A[0][6.06] = 0.2401\n",
        "    B[0][6.06] = 75.22\n",
        "    A[0][12.12] = 0.2458\n",
        "    B[0][12.12] = 57.76\n",
        "\n",
        "    for layer in layers:\n",
        "        for cell_size in cell_sizes:\n",
        "            for cell_num in range(25):\n",
        "                e_calib = df.loc[df.cell_size == cell_size, 'l{}_{}'.format(layer, cell_num)] * A[layer][cell_size] + \\\n",
        "                        B[layer][cell_size]\n",
        "                df.loc[df.cell_size == cell_size, 'l{}_{}'.format(layer, cell_num)] = \\\n",
        "                    np.clip(e_calib, a_min=0, a_max=None)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ckSsnX8w3XgJ"
      },
      "outputs": [],
      "source": [
        "nPV_start_time = datetime.now()\n",
        "data_train = pd.read_csv(INPUT_PATH)#[:N_SIGNALS]\n",
        "\n",
        "data_train = data_train[~np.isnan(data_train.l0_12.values)].reset_index(drop=True)\n",
        "\n",
        "data_train = calib(data_train)\n",
        "\n",
        "data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "data_train.fillna(0, inplace=True)\n",
        "\n",
        "data_train['p_ECAL'] = data_train['eKinetic'].values #* 1000\n",
        "data_train['t_ECAL'] = data_train.timing.values\n",
        "\n",
        "data_train['x_ECAL'] = data_train.entry_x.values\n",
        "data_train['y_ECAL'] = data_train.entry_y.values\n",
        "data_train['z_ECAL'] = data_train.entry_z.values\n",
        "\n",
        "data_train = data_train[data_train['p_ECAL'] < 100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HpYe-Ilf3cTC"
      },
      "outputs": [],
      "source": [
        "for layer in layers:\n",
        "    data_train['l{}_sum_5x5_rec'.format(layer)] = 0.0\n",
        "\n",
        "    for cell_size in cell_sizes_cm:\n",
        "        sum_of_cluster = data_train[data_train.cell_size == cell_size][['l{}_{}'.format(layer, i) for i in range(25)]].values.sum(axis=-1)\n",
        "        corr_coef = np.nanmean((data_train[data_train.cell_size == cell_size].p_ECAL.values)) / np.nanmean(sum_of_cluster)\n",
        "        data_train.loc[data_train.cell_size == cell_size, 'l{}_sum_5x5_rec'.format(layer)] = sum_of_cluster * corr_coef\n",
        "\n",
        "get_features(data_train)\n",
        "\n",
        "data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "data_train.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0RU4DU9Y7SqA"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        " \n",
        "  def __init__(self,X,y):\n",
        "    self.X = torch.tensor(np.array(X).reshape(len(X),2,5,5),dtype=torch.float32)\n",
        "    self.y = torch.tensor(np.array(y),dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.X[idx],self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "gIJ_wlmI3hcf",
        "outputId": "49f30755-b18f-4ffc-8178-884ba3cb005c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        runNumber  evtNumber  evtIndex  prod_vertex_x  prod_vertex_y  \\\n",
              "0             666       6051         0    -830.238837     -43.902127   \n",
              "1             666       6051         0    -830.238804     -43.902134   \n",
              "2             666       6051         0      34.845277      31.003130   \n",
              "3             666       6051         0      -0.024234      -0.021201   \n",
              "4             666       6051         0      -0.024234      -0.021201   \n",
              "...           ...        ...       ...            ...            ...   \n",
              "642553        666       7250       999    -236.261996     148.632678   \n",
              "642554        666       7250       999       0.024279       0.027073   \n",
              "642555        666       7250       999       0.024279       0.027073   \n",
              "642556        666       7250       999    -733.309265    -279.444304   \n",
              "642557        666       7250       999    -733.309265    -279.444304   \n",
              "\n",
              "        prod_vertex_z      entry_x     entry_y  entry_z          px  ...  \\\n",
              "0        12039.559208  -898.586070  -27.695497  12620.0 -478.642784  ...   \n",
              "1        12039.559397  -869.766946  -35.421831  12620.0 -322.870145  ...   \n",
              "2         4631.168135   547.022763 -236.376609  12620.0  225.805553  ...   \n",
              "3          -59.544035  -168.658536 -484.246651  12620.0  -41.986119  ...   \n",
              "4          -59.544035 -1253.181624  729.837446  12620.0 -292.437384  ...   \n",
              "...               ...          ...         ...      ...         ...  ...   \n",
              "642553   12017.227336  -355.565642  112.409183  12620.0 -698.361085  ...   \n",
              "642554       7.349056  -174.369309  325.427129  12620.0  -29.465579  ...   \n",
              "642555       7.349056  1513.867338 -254.578276  12620.0  560.097329  ...   \n",
              "642556   12041.901158  -819.393502 -269.177856  12620.0 -417.707427  ...   \n",
              "642557   12041.901158  -829.495153 -290.159367  12620.0 -669.071329  ...   \n",
              "\n",
              "        l0_x_bar_cal  l0_xRec_loc  l0_xGen_loc  l0_y_bar  entry_y_new  \\\n",
              "0         -89.110626     0.274374    -1.651113 -0.106242   -24.903372   \n",
              "1         -89.110626     0.274374     1.727303  2.923758   -33.960819   \n",
              "2          55.981817    -0.073183    -0.711607  1.173273  -239.723529   \n",
              "3         -13.828647    -0.193647    -3.363851  5.334230  -488.065601   \n",
              "4        -124.761414    -1.541414    -3.778323 -5.881285   739.622969   \n",
              "...              ...          ...          ...       ...          ...   \n",
              "642553    -35.624419    -0.021919    -1.339538 -0.069530   108.202548   \n",
              "642554    -16.862122     0.560378    -0.111219 -1.705312   327.233093   \n",
              "642555    151.370544    -0.129456     1.927172 -2.650468  -258.009982   \n",
              "642556    -82.048651     1.276349    -0.103442 -2.001970  -267.401958   \n",
              "642557    -82.048651     1.276349    -1.288347  1.028030  -292.012867   \n",
              "\n",
              "        l0_y_bar_cal  l0_yRec_loc  l0_yGen_loc  t0_weighted_3x3  \\\n",
              "0          -1.621242    -0.106242    -0.975337     8.604970e+01   \n",
              "1          -1.621242    -0.106242    -1.881082     8.604970e+01   \n",
              "2         -21.551727     1.173273    -1.247353     8.600710e+01   \n",
              "3         -41.630770    -0.725770    -7.901560     8.590364e+01   \n",
              "4          68.858715    -1.841285     3.262297    -1.788332e+36   \n",
              "...              ...          ...          ...              ...   \n",
              "642553     11.292970    -0.069530    -0.542245     8.594869e+01   \n",
              "642554     30.867188    -0.190312     1.665809     8.563599e+01   \n",
              "642555    -28.910468     1.389532     4.499002    -5.194555e+36   \n",
              "642556    -27.756970     1.028030     2.044804     8.616513e+01   \n",
              "642557    -27.756970     1.028030    -0.416287     8.616513e+01   \n",
              "\n",
              "        t0_weighted_5x5  \n",
              "0          8.602384e+01  \n",
              "1          8.602384e+01  \n",
              "2          8.605714e+01  \n",
              "3          8.584514e+01  \n",
              "4         -5.651775e+36  \n",
              "...                 ...  \n",
              "642553     8.595159e+01  \n",
              "642554     8.565155e+01  \n",
              "642555    -1.651027e+37  \n",
              "642556     8.626701e+01  \n",
              "642557     8.626701e+01  \n",
              "\n",
              "[642558 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4520ea57-21e2-4bd7-808c-986aff674fb2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>runNumber</th>\n",
              "      <th>evtNumber</th>\n",
              "      <th>evtIndex</th>\n",
              "      <th>prod_vertex_x</th>\n",
              "      <th>prod_vertex_y</th>\n",
              "      <th>prod_vertex_z</th>\n",
              "      <th>entry_x</th>\n",
              "      <th>entry_y</th>\n",
              "      <th>entry_z</th>\n",
              "      <th>px</th>\n",
              "      <th>...</th>\n",
              "      <th>l0_x_bar_cal</th>\n",
              "      <th>l0_xRec_loc</th>\n",
              "      <th>l0_xGen_loc</th>\n",
              "      <th>l0_y_bar</th>\n",
              "      <th>entry_y_new</th>\n",
              "      <th>l0_y_bar_cal</th>\n",
              "      <th>l0_yRec_loc</th>\n",
              "      <th>l0_yGen_loc</th>\n",
              "      <th>t0_weighted_3x3</th>\n",
              "      <th>t0_weighted_5x5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>666</td>\n",
              "      <td>6051</td>\n",
              "      <td>0</td>\n",
              "      <td>-830.238837</td>\n",
              "      <td>-43.902127</td>\n",
              "      <td>12039.559208</td>\n",
              "      <td>-898.586070</td>\n",
              "      <td>-27.695497</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-478.642784</td>\n",
              "      <td>...</td>\n",
              "      <td>-89.110626</td>\n",
              "      <td>0.274374</td>\n",
              "      <td>-1.651113</td>\n",
              "      <td>-0.106242</td>\n",
              "      <td>-24.903372</td>\n",
              "      <td>-1.621242</td>\n",
              "      <td>-0.106242</td>\n",
              "      <td>-0.975337</td>\n",
              "      <td>8.604970e+01</td>\n",
              "      <td>8.602384e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>666</td>\n",
              "      <td>6051</td>\n",
              "      <td>0</td>\n",
              "      <td>-830.238804</td>\n",
              "      <td>-43.902134</td>\n",
              "      <td>12039.559397</td>\n",
              "      <td>-869.766946</td>\n",
              "      <td>-35.421831</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-322.870145</td>\n",
              "      <td>...</td>\n",
              "      <td>-89.110626</td>\n",
              "      <td>0.274374</td>\n",
              "      <td>1.727303</td>\n",
              "      <td>2.923758</td>\n",
              "      <td>-33.960819</td>\n",
              "      <td>-1.621242</td>\n",
              "      <td>-0.106242</td>\n",
              "      <td>-1.881082</td>\n",
              "      <td>8.604970e+01</td>\n",
              "      <td>8.602384e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>666</td>\n",
              "      <td>6051</td>\n",
              "      <td>0</td>\n",
              "      <td>34.845277</td>\n",
              "      <td>31.003130</td>\n",
              "      <td>4631.168135</td>\n",
              "      <td>547.022763</td>\n",
              "      <td>-236.376609</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>225.805553</td>\n",
              "      <td>...</td>\n",
              "      <td>55.981817</td>\n",
              "      <td>-0.073183</td>\n",
              "      <td>-0.711607</td>\n",
              "      <td>1.173273</td>\n",
              "      <td>-239.723529</td>\n",
              "      <td>-21.551727</td>\n",
              "      <td>1.173273</td>\n",
              "      <td>-1.247353</td>\n",
              "      <td>8.600710e+01</td>\n",
              "      <td>8.605714e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>666</td>\n",
              "      <td>6051</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.024234</td>\n",
              "      <td>-0.021201</td>\n",
              "      <td>-59.544035</td>\n",
              "      <td>-168.658536</td>\n",
              "      <td>-484.246651</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-41.986119</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.828647</td>\n",
              "      <td>-0.193647</td>\n",
              "      <td>-3.363851</td>\n",
              "      <td>5.334230</td>\n",
              "      <td>-488.065601</td>\n",
              "      <td>-41.630770</td>\n",
              "      <td>-0.725770</td>\n",
              "      <td>-7.901560</td>\n",
              "      <td>8.590364e+01</td>\n",
              "      <td>8.584514e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>666</td>\n",
              "      <td>6051</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.024234</td>\n",
              "      <td>-0.021201</td>\n",
              "      <td>-59.544035</td>\n",
              "      <td>-1253.181624</td>\n",
              "      <td>729.837446</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-292.437384</td>\n",
              "      <td>...</td>\n",
              "      <td>-124.761414</td>\n",
              "      <td>-1.541414</td>\n",
              "      <td>-3.778323</td>\n",
              "      <td>-5.881285</td>\n",
              "      <td>739.622969</td>\n",
              "      <td>68.858715</td>\n",
              "      <td>-1.841285</td>\n",
              "      <td>3.262297</td>\n",
              "      <td>-1.788332e+36</td>\n",
              "      <td>-5.651775e+36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642553</th>\n",
              "      <td>666</td>\n",
              "      <td>7250</td>\n",
              "      <td>999</td>\n",
              "      <td>-236.261996</td>\n",
              "      <td>148.632678</td>\n",
              "      <td>12017.227336</td>\n",
              "      <td>-355.565642</td>\n",
              "      <td>112.409183</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-698.361085</td>\n",
              "      <td>...</td>\n",
              "      <td>-35.624419</td>\n",
              "      <td>-0.021919</td>\n",
              "      <td>-1.339538</td>\n",
              "      <td>-0.069530</td>\n",
              "      <td>108.202548</td>\n",
              "      <td>11.292970</td>\n",
              "      <td>-0.069530</td>\n",
              "      <td>-0.542245</td>\n",
              "      <td>8.594869e+01</td>\n",
              "      <td>8.595159e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642554</th>\n",
              "      <td>666</td>\n",
              "      <td>7250</td>\n",
              "      <td>999</td>\n",
              "      <td>0.024279</td>\n",
              "      <td>0.027073</td>\n",
              "      <td>7.349056</td>\n",
              "      <td>-174.369309</td>\n",
              "      <td>325.427129</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-29.465579</td>\n",
              "      <td>...</td>\n",
              "      <td>-16.862122</td>\n",
              "      <td>0.560378</td>\n",
              "      <td>-0.111219</td>\n",
              "      <td>-1.705312</td>\n",
              "      <td>327.233093</td>\n",
              "      <td>30.867188</td>\n",
              "      <td>-0.190312</td>\n",
              "      <td>1.665809</td>\n",
              "      <td>8.563599e+01</td>\n",
              "      <td>8.565155e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642555</th>\n",
              "      <td>666</td>\n",
              "      <td>7250</td>\n",
              "      <td>999</td>\n",
              "      <td>0.024279</td>\n",
              "      <td>0.027073</td>\n",
              "      <td>7.349056</td>\n",
              "      <td>1513.867338</td>\n",
              "      <td>-254.578276</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>560.097329</td>\n",
              "      <td>...</td>\n",
              "      <td>151.370544</td>\n",
              "      <td>-0.129456</td>\n",
              "      <td>1.927172</td>\n",
              "      <td>-2.650468</td>\n",
              "      <td>-258.009982</td>\n",
              "      <td>-28.910468</td>\n",
              "      <td>1.389532</td>\n",
              "      <td>4.499002</td>\n",
              "      <td>-5.194555e+36</td>\n",
              "      <td>-1.651027e+37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642556</th>\n",
              "      <td>666</td>\n",
              "      <td>7250</td>\n",
              "      <td>999</td>\n",
              "      <td>-733.309265</td>\n",
              "      <td>-279.444304</td>\n",
              "      <td>12041.901158</td>\n",
              "      <td>-819.393502</td>\n",
              "      <td>-269.177856</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-417.707427</td>\n",
              "      <td>...</td>\n",
              "      <td>-82.048651</td>\n",
              "      <td>1.276349</td>\n",
              "      <td>-0.103442</td>\n",
              "      <td>-2.001970</td>\n",
              "      <td>-267.401958</td>\n",
              "      <td>-27.756970</td>\n",
              "      <td>1.028030</td>\n",
              "      <td>2.044804</td>\n",
              "      <td>8.616513e+01</td>\n",
              "      <td>8.626701e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642557</th>\n",
              "      <td>666</td>\n",
              "      <td>7250</td>\n",
              "      <td>999</td>\n",
              "      <td>-733.309265</td>\n",
              "      <td>-279.444304</td>\n",
              "      <td>12041.901158</td>\n",
              "      <td>-829.495153</td>\n",
              "      <td>-290.159367</td>\n",
              "      <td>12620.0</td>\n",
              "      <td>-669.071329</td>\n",
              "      <td>...</td>\n",
              "      <td>-82.048651</td>\n",
              "      <td>1.276349</td>\n",
              "      <td>-1.288347</td>\n",
              "      <td>1.028030</td>\n",
              "      <td>-292.012867</td>\n",
              "      <td>-27.756970</td>\n",
              "      <td>1.028030</td>\n",
              "      <td>-0.416287</td>\n",
              "      <td>8.616513e+01</td>\n",
              "      <td>8.626701e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>642558 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4520ea57-21e2-4bd7-808c-986aff674fb2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4520ea57-21e2-4bd7-808c-986aff674fb2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4520ea57-21e2-4bd7-808c-986aff674fb2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "An-UFQAP7mO3"
      },
      "outputs": [],
      "source": [
        "train_features = []\n",
        "train_features += ['t0_{}'.format(i) for i in range(25)]    \n",
        "for layer in layers:\n",
        "    train_features += \\\n",
        "        ['l{}_{}'.format(layer, i) for i in range(25)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cell_size = cell_sizes_cm[4]"
      ],
      "metadata": {
        "id": "GrXIppJFHlSe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4CnePrlR7oIG"
      },
      "outputs": [],
      "source": [
        "data = data_train[data_train['cell_size'] == cell_size][train_features]\n",
        "target =  data_train[data_train['cell_size'] == cell_size]['t_ECAL'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw8gwf2u40nQ"
      },
      "source": [
        "## Modelling ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0qZpM4yf6au0"
      },
      "outputs": [],
      "source": [
        "def conv_block(input_size, output_size, kernel_size):\n",
        "    block = nn.Sequential(\n",
        "        nn.Conv2d(input_size, output_size, kernel_size), nn.ReLU(), nn.BatchNorm2d(output_size),# nn.Dropout(0.2),\n",
        "    )\n",
        "\n",
        "    return block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ckoEseY43bZ",
        "outputId": "743bb3d3-fed8-4232-ac2f-eba8551f2cb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv_block1): Sequential(\n",
              "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block2): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block3): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (relu1): LeakyReLU(negative_slope=0.15)\n",
              "  (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
              "  (relu2): LeakyReLU(negative_slope=0.1)\n",
              "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear3): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (drop): Dropout(p=0.35, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.conv_block1 = conv_block(2,128,(3,3))\n",
        "        self.conv_block2 = conv_block(128,256,(3,3))\n",
        "        self.conv_block3 = conv_block(256,256,(1,1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(256, 512)\n",
        "        self.relu1 = nn.LeakyReLU(0.15)\n",
        "        self.linear2 = nn.Linear(512, 64)\n",
        "        self.relu2 = nn.LeakyReLU(0.1)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.linear3 = nn.Linear(64, 1)\n",
        "        self.drop = nn.Dropout(0.35)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.bn1(x)\n",
        "        #x = self.drop(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.linear3(x)\n",
        "        return self.relu2(x)\n",
        "\n",
        "def init_weights(self):\n",
        "      for m in self.modules():\n",
        "          if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "              nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "              if m.bias is not None:\n",
        "                  nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(50,16)\n",
        "        self.linear2 = nn.Linear(16,8)\n",
        "        self.linear3 = nn.Linear(8,1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.bn2 = nn.BatchNorm1d(8)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VXbeC8GI52bx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0H-OgLQdn84"
      },
      "source": [
        "## Reconstructing time ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "g69TpZQHwY7B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6vSGy4hdYAiU"
      },
      "outputs": [],
      "source": [
        "data['target'] = target\n",
        "X_train,X_val,y_train,y_val = train_test_split(data.drop('target',axis=1), data.target, random_state=1, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_data = StandardScaler()\n",
        "scaler_target = StandardScaler()\n",
        "\n",
        "X_train = scaler_data.fit_transform(X_train)\n",
        "#y_train = scaler_target.fit_transform(y_train.values.reshape(-1,1))\n",
        "\n",
        "X_val = scaler_data.transform(X_val)\n",
        "\n",
        "mu_y = y_train.mean()\n",
        "std_y = y_train.std()\n",
        "y_train -= mu_y\n",
        "y_train /= std_y\n",
        "\n",
        "# y_val -= mu_y\n",
        "# y_val /= std_y"
      ],
      "metadata": {
        "id": "S6KAopmhwXiI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "_YTizK6zj5a8",
        "outputId": "73f6555d-81d6-4734-c018-fbb3d9ef5d62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 105., 1489., 2917., 2053.,  941.,  222.,   40.,    9.,    3.,\n",
              "           3.]),\n",
              " array([-2.72144594, -1.78612188, -0.85079783,  0.08452623,  1.01985028,\n",
              "         1.95517433,  2.89049839,  3.82582244,  4.76114649,  5.69647055,\n",
              "         6.6317946 ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.hist(y_val)\n",
        "plt.hist(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EkRe_Rtm6r_R"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "H_UHaMa6UPIT",
        "outputId": "41e8575f-77d3-496c-ae52-76905c15fd9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fed3f4f4fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_train, color = 'red',bins=50, label = 'Train Target')\n",
        "#plt.hist(y_train_resampled, color = 'blue', bins=50)\n",
        "plt.hist((y_val - mu_y)/std_y, color = 'orange',bins=50, label = 'Validation Target')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "M2EEpJ3m8w0l"
      },
      "outputs": [],
      "source": [
        "train_set = MyDataset(X_train, y_train)\n",
        "val_set = MyDataset(X_val, y_val)\n",
        "\n",
        "training_loader = DataLoader(train_set, batch_size = 1024)\n",
        "validation_loader = DataLoader(val_set, batch_size = 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RaJlp4mSoB6v"
      },
      "outputs": [],
      "source": [
        "def early_stopping(val_loss, patiance = 5):\n",
        "  counter = 0.\n",
        "  for v in range(patiance):\n",
        "    if val_loss[-v] > val_loss[-v-1]:\n",
        "      counter += 1.\n",
        "    else:\n",
        "      False\n",
        "\n",
        "  if patiance == counter:\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ktZJIq2B-wLL"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = [] \n",
        "\n",
        "def train_regression_model(model, criterion,  val_crit, optimizer, train_loader, val_loader, num_epochs, patiance = 5):\n",
        "    # set up the device to use for training\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    best_model = 0.\n",
        "    best_validation = float('inf')\n",
        "    #print(mu_y, std_y)\n",
        "\n",
        "    # train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            # move the inputs and labels to the device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs*std_y + mu_y, labels.squeeze()*std_y + mu_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            #running_loss += mean_squared_error(scaler_target.inverse_transform(outputs.cpu().detach().numpy().reshape(-1, 1)), scaler_target.inverse_transform(labels.cpu().detach().numpy().reshape(-1, 1))).item() \n",
        "\n",
        "       \n",
        "        epoch_loss = np.sqrt(running_loss / len(train_loader))  # calculate the average epoch loss\n",
        "        running_loss = 0\n",
        "        train_losses.append(epoch_loss)\n",
        "        print(f\"Average training loss for epoch {epoch+1}: {epoch_loss:.5f}\")\n",
        "\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader, 0):\n",
        "                inputs, labels = data\n",
        "\n",
        "                # move the inputs and labels to the device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                outputs = model(inputs).squeeze() \n",
        "                loss = val_crit(outputs*std_y + mu_y, labels.squeeze())\n",
        "\n",
        "                # accumulate validation loss\n",
        "                #loss += mean_squared_error(scaler_target.inverse_transform(outputs.cpu().detach().numpy().reshape(-1, 1)), \n",
        "                #                              labels.cpu().detach().numpy().reshape(-1, 1)).item()\n",
        "                \n",
        "                running_val_loss += loss.item()\n",
        "        val_loss = np.sqrt(running_val_loss / len(val_loader))\n",
        "        if val_loss < best_validation:\n",
        "          best_model = model\n",
        "          best_validation = val_loss\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "        print(f\"Average validation loss for epoch {epoch+1}: {val_loss:.5f}\")\n",
        "       \n",
        "        if epoch > patiance:\n",
        "          if early_stopping(val_losses,patiance):\n",
        "            print('Early stopping')\n",
        "            break\n",
        "      \n",
        "    torch.save(model.state_dict(), '/content/attempt1/epoch'+str(epoch))\n",
        "    return best_model\n",
        "    print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-gzTAJM3gsmO"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.MSELoss()\n",
        "#loss_fn = my_loss\n",
        "val_loss = torch.nn.MSELoss()\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)#,weight_decay = 0.000001)\n",
        "train_losses = []\n",
        "val_losses = [] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUv2Dty2-_zB",
        "outputId": "ddb103a6-4961-40a6-fab5-efcc442b9b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss for epoch 1: 0.27085\n",
            "Average validation loss for epoch 1: 0.30630\n",
            "Average training loss for epoch 2: 0.29083\n",
            "Average validation loss for epoch 2: 0.35367\n",
            "Average training loss for epoch 3: 0.27868\n",
            "Average validation loss for epoch 3: 0.29226\n",
            "Average training loss for epoch 4: 0.25979\n",
            "Average validation loss for epoch 4: 0.29167\n",
            "Average training loss for epoch 5: 0.27836\n",
            "Average validation loss for epoch 5: 0.31308\n",
            "Average training loss for epoch 6: 0.26854\n",
            "Average validation loss for epoch 6: 0.29361\n",
            "Average training loss for epoch 7: 0.26771\n",
            "Average validation loss for epoch 7: 0.32794\n",
            "Average training loss for epoch 8: 0.27211\n",
            "Average validation loss for epoch 8: 0.27990\n",
            "Average training loss for epoch 9: 0.27064\n",
            "Average validation loss for epoch 9: 0.28174\n",
            "Average training loss for epoch 10: 0.26881\n",
            "Average validation loss for epoch 10: 0.35814\n",
            "Average training loss for epoch 11: 0.24562\n",
            "Average validation loss for epoch 11: 0.30038\n",
            "Average training loss for epoch 12: 0.25630\n",
            "Average validation loss for epoch 12: 0.33059\n",
            "Average training loss for epoch 13: 0.26593\n",
            "Average validation loss for epoch 13: 0.29001\n",
            "Average training loss for epoch 14: 0.26395\n",
            "Average validation loss for epoch 14: 0.30198\n",
            "Average training loss for epoch 15: 0.26794\n",
            "Average validation loss for epoch 15: 0.29258\n",
            "Average training loss for epoch 16: 0.25354\n",
            "Average validation loss for epoch 16: 0.29505\n",
            "Average training loss for epoch 17: 0.23954\n",
            "Average validation loss for epoch 17: 0.26992\n",
            "Average training loss for epoch 18: 0.24173\n",
            "Average validation loss for epoch 18: 0.37368\n",
            "Average training loss for epoch 19: 0.25393\n",
            "Average validation loss for epoch 19: 0.26873\n",
            "Average training loss for epoch 20: 0.23565\n",
            "Average validation loss for epoch 20: 0.26823\n",
            "Average training loss for epoch 21: 0.23871\n",
            "Average validation loss for epoch 21: 0.27139\n",
            "Average training loss for epoch 22: 0.24686\n",
            "Average validation loss for epoch 22: 0.28228\n",
            "Average training loss for epoch 23: 0.25467\n",
            "Average validation loss for epoch 23: 0.27513\n",
            "Average training loss for epoch 24: 0.25904\n",
            "Average validation loss for epoch 24: 0.27277\n",
            "Average training loss for epoch 25: 0.24534\n",
            "Average validation loss for epoch 25: 0.27656\n",
            "Average training loss for epoch 26: 0.25702\n",
            "Average validation loss for epoch 26: 0.28456\n",
            "Average training loss for epoch 27: 0.24165\n",
            "Average validation loss for epoch 27: 0.29880\n",
            "Average training loss for epoch 28: 0.23971\n",
            "Average validation loss for epoch 28: 0.26893\n",
            "Average training loss for epoch 29: 0.23990\n",
            "Average validation loss for epoch 29: 0.27400\n",
            "Average training loss for epoch 30: 0.23461\n",
            "Average validation loss for epoch 30: 0.25845\n",
            "Average training loss for epoch 31: 0.21966\n",
            "Average validation loss for epoch 31: 0.25390\n",
            "Average training loss for epoch 32: 0.22386\n",
            "Average validation loss for epoch 32: 0.28850\n",
            "Average training loss for epoch 33: 0.25798\n",
            "Average validation loss for epoch 33: 0.27202\n",
            "Average training loss for epoch 34: 0.22874\n",
            "Average validation loss for epoch 34: 0.27232\n",
            "Average training loss for epoch 35: 0.28950\n",
            "Average validation loss for epoch 35: 0.25520\n",
            "Average training loss for epoch 36: 0.23888\n",
            "Average validation loss for epoch 36: 0.26542\n",
            "Average training loss for epoch 37: 0.21884\n",
            "Average validation loss for epoch 37: 0.26944\n",
            "Average training loss for epoch 38: 0.21561\n",
            "Average validation loss for epoch 38: 0.28139\n",
            "Average training loss for epoch 39: 0.22538\n",
            "Average validation loss for epoch 39: 0.25670\n",
            "Average training loss for epoch 40: 0.22946\n",
            "Average validation loss for epoch 40: 0.26116\n",
            "Average training loss for epoch 41: 0.20992\n",
            "Average validation loss for epoch 41: 0.26136\n",
            "Average training loss for epoch 42: 0.20892\n",
            "Average validation loss for epoch 42: 0.25805\n",
            "Average training loss for epoch 43: 0.20495\n",
            "Average validation loss for epoch 43: 0.26251\n",
            "Average training loss for epoch 44: 0.22075\n",
            "Average validation loss for epoch 44: 0.26583\n",
            "Average training loss for epoch 45: 0.20182\n",
            "Average validation loss for epoch 45: 0.26623\n",
            "Average training loss for epoch 46: 0.20157\n",
            "Average validation loss for epoch 46: 0.26787\n",
            "Average training loss for epoch 47: 0.20108\n",
            "Average validation loss for epoch 47: 0.27378\n",
            "Average training loss for epoch 48: 0.20454\n",
            "Average validation loss for epoch 48: 0.25425\n",
            "Average training loss for epoch 49: 0.19680\n",
            "Average validation loss for epoch 49: 0.25665\n",
            "Average training loss for epoch 50: 0.19334\n",
            "Average validation loss for epoch 50: 0.27084\n",
            "Average training loss for epoch 51: 0.20206\n",
            "Average validation loss for epoch 51: 0.25632\n",
            "Average training loss for epoch 52: 0.19505\n",
            "Average validation loss for epoch 52: 0.26072\n",
            "Average training loss for epoch 53: 0.19350\n",
            "Average validation loss for epoch 53: 0.35288\n",
            "Average training loss for epoch 54: 0.20344\n",
            "Average validation loss for epoch 54: 0.26882\n",
            "Average training loss for epoch 55: 0.19837\n",
            "Average validation loss for epoch 55: 0.26998\n",
            "Average training loss for epoch 56: 0.19763\n",
            "Average validation loss for epoch 56: 0.26579\n",
            "Average training loss for epoch 57: 0.19499\n",
            "Average validation loss for epoch 57: 0.26033\n",
            "Average training loss for epoch 58: 0.18987\n",
            "Average validation loss for epoch 58: 0.26986\n",
            "Average training loss for epoch 59: 0.18775\n",
            "Average validation loss for epoch 59: 0.26710\n",
            "Average training loss for epoch 60: 0.18900\n",
            "Average validation loss for epoch 60: 0.26658\n",
            "Average training loss for epoch 61: 0.18288\n",
            "Average validation loss for epoch 61: 0.28891\n",
            "Average training loss for epoch 62: 0.19204\n",
            "Average validation loss for epoch 62: 0.26001\n",
            "Average training loss for epoch 63: 0.18692\n",
            "Average validation loss for epoch 63: 0.26727\n",
            "Average training loss for epoch 64: 0.18172\n",
            "Average validation loss for epoch 64: 0.26613\n",
            "Average training loss for epoch 65: 0.18518\n",
            "Average validation loss for epoch 65: 0.25846\n",
            "Average training loss for epoch 66: 0.18030\n",
            "Average validation loss for epoch 66: 0.27112\n",
            "Average training loss for epoch 67: 0.18026\n",
            "Average validation loss for epoch 67: 0.25702\n",
            "Average training loss for epoch 68: 0.18525\n",
            "Average validation loss for epoch 68: 0.26528\n",
            "Average training loss for epoch 69: 0.18552\n",
            "Average validation loss for epoch 69: 0.28619\n",
            "Average training loss for epoch 70: 0.18518\n",
            "Average validation loss for epoch 70: 0.26215\n",
            "Average training loss for epoch 71: 0.18274\n",
            "Average validation loss for epoch 71: 0.29062\n",
            "Average training loss for epoch 72: 0.17981\n",
            "Average validation loss for epoch 72: 0.27076\n",
            "Average training loss for epoch 73: 0.17485\n",
            "Average validation loss for epoch 73: 0.27463\n",
            "Average training loss for epoch 74: 0.18363\n",
            "Average validation loss for epoch 74: 0.27819\n",
            "Average training loss for epoch 75: 0.17874\n",
            "Average validation loss for epoch 75: 0.26397\n",
            "Average training loss for epoch 76: 0.17468\n",
            "Average validation loss for epoch 76: 0.27357\n",
            "Average training loss for epoch 77: 0.17015\n",
            "Average validation loss for epoch 77: 0.25962\n",
            "Average training loss for epoch 78: 0.16876\n",
            "Average validation loss for epoch 78: 0.26568\n",
            "Average training loss for epoch 79: 0.17983\n",
            "Average validation loss for epoch 79: 0.26553\n",
            "Average training loss for epoch 80: 0.17650\n",
            "Average validation loss for epoch 80: 0.26954\n",
            "Average training loss for epoch 81: 0.17700\n",
            "Average validation loss for epoch 81: 0.26556\n",
            "Average training loss for epoch 82: 0.17746\n",
            "Average validation loss for epoch 82: 0.26976\n",
            "Average training loss for epoch 83: 0.16970\n",
            "Average validation loss for epoch 83: 0.27653\n",
            "Average training loss for epoch 84: 0.17383\n",
            "Average validation loss for epoch 84: 0.26789\n",
            "Average training loss for epoch 85: 0.16109\n",
            "Average validation loss for epoch 85: 0.25808\n",
            "Average training loss for epoch 86: 0.16055\n",
            "Average validation loss for epoch 86: 0.27709\n",
            "Average training loss for epoch 87: 0.17322\n",
            "Average validation loss for epoch 87: 0.26937\n",
            "Average training loss for epoch 88: 0.17071\n",
            "Average validation loss for epoch 88: 0.26592\n",
            "Average training loss for epoch 89: 0.16903\n",
            "Average validation loss for epoch 89: 0.27094\n",
            "Average training loss for epoch 90: 0.16807\n",
            "Average validation loss for epoch 90: 0.27021\n",
            "Average training loss for epoch 91: 0.16329\n",
            "Average validation loss for epoch 91: 0.27807\n",
            "Average training loss for epoch 92: 0.16520\n",
            "Average validation loss for epoch 92: 0.27355\n",
            "Average training loss for epoch 93: 0.16553\n",
            "Average validation loss for epoch 93: 0.26945\n",
            "Average training loss for epoch 94: 0.16130\n",
            "Average validation loss for epoch 94: 0.26916\n",
            "Average training loss for epoch 95: 0.16025\n",
            "Average validation loss for epoch 95: 0.26755\n",
            "Average training loss for epoch 96: 0.15770\n",
            "Average validation loss for epoch 96: 0.26680\n",
            "Average training loss for epoch 97: 0.17950\n",
            "Average validation loss for epoch 97: 0.26836\n",
            "Average training loss for epoch 98: 0.15707\n",
            "Average validation loss for epoch 98: 0.27193\n",
            "Average training loss for epoch 99: 0.15687\n",
            "Average validation loss for epoch 99: 0.26541\n",
            "Average training loss for epoch 100: 0.15262\n",
            "Average validation loss for epoch 100: 0.26503\n"
          ]
        }
      ],
      "source": [
        "best_model = train_regression_model(model, loss_fn, val_loss, optimizer, training_loader, validation_loader, num_epochs=100, patiance = 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses[:], color = 'orange', label = 'Train Loss')\n",
        "plt.plot(val_losses[:], color = 'blue', label = 'Validation Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "MGEna3GHhy7c",
        "outputId": "5529fba1-2192-4e95-89ef-0c31e6c8488a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fed3f06e1a0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "beyH8Aj4zlu3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def experiment(architecture, n_splits = 5):\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, random_state=22, shuffle=True)\n",
        "    kf.get_n_splits(data)\n",
        "\n",
        "    predict_train = np.zeros(len(data))\n",
        "\n",
        "    for train_index, val_index in kf.split(data):\n",
        "\n",
        "        if architecture == 'cnn':\n",
        "          model = CNN()\n",
        "          model.apply(init_weights)\n",
        "        elif architecture == 'dnn':\n",
        "          model = DNN()\n",
        "          model.apply(init_weights)\n",
        "\n",
        "        X_train = data.iloc[train_index].drop('target',axis=1)\n",
        "        X_val = data.iloc[val_index].drop('target',axis=1)\n",
        "\n",
        "        y_train = data.iloc[train_index]['target']\n",
        "        y_val = data.iloc[val_index]['target']\n",
        "\n",
        "\n",
        "        scaler_data = StandardScaler()\n",
        "        scaler_target = StandardScaler()\n",
        "\n",
        "        X_train = scaler_data.fit_transform(X_train)\n",
        "\n",
        "        X_val = scaler_data.transform(X_val)\n",
        "\n",
        "        mu_y = y_train.mean()\n",
        "        std_y = y_train.std()\n",
        "        y_train -= mu_y\n",
        "        y_train /= std_y\n",
        "\n",
        "\n",
        "        train_set = MyDataset(X_train,y_train)\n",
        "        val_set = MyDataset(X_val,y_val)\n",
        "\n",
        "        training_loader = DataLoader(train_set,batch_size = 512,)\n",
        "        validation_loader = DataLoader(val_set,batch_size = 512,)\n",
        "        \n",
        "\n",
        "\n",
        "        val_loss = torch.nn.MSELoss()\n",
        "        loss_fn = torch.nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.05)#,weight_decay = 0.000001)\n",
        "\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = [] \n",
        "        best_model = train_regression_model(model, loss_fn, val_loss, optimizer, training_loader, validation_loader, num_epochs=30, patiance = 15)\n",
        "\n",
        "\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        preds = []\n",
        "        for X,y in validation_loader:\n",
        "          predictions = best_model(X.to(device))\n",
        "          predictions = predictions*std_y + mu_y\n",
        "          for pred in predictions:\n",
        "            preds.append(pred.item())\n",
        "\n",
        "        predict_train[val_index] = np.array(preds)\n",
        "\n",
        "\n",
        "    return predict_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_preds = experiment('cnn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG70agti8MNC",
        "outputId": "68c14b18-925e-498b-b836-a87c5afd52ef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss for epoch 1: 1.63762\n",
            "Average validation loss for epoch 1: 1.04361\n",
            "Average training loss for epoch 2: 0.80850\n",
            "Average validation loss for epoch 2: 0.53061\n",
            "Average training loss for epoch 3: 0.50809\n",
            "Average validation loss for epoch 3: 0.43654\n",
            "Average training loss for epoch 4: 0.38237\n",
            "Average validation loss for epoch 4: 0.32393\n",
            "Average training loss for epoch 5: 0.35431\n",
            "Average validation loss for epoch 5: 0.33740\n",
            "Average training loss for epoch 6: 0.31479\n",
            "Average validation loss for epoch 6: 0.30255\n",
            "Average training loss for epoch 7: 0.28425\n",
            "Average validation loss for epoch 7: 0.25657\n",
            "Average training loss for epoch 8: 0.27185\n",
            "Average validation loss for epoch 8: 0.27453\n",
            "Average training loss for epoch 9: 0.26545\n",
            "Average validation loss for epoch 9: 0.24993\n",
            "Average training loss for epoch 10: 0.30246\n",
            "Average validation loss for epoch 10: 0.27624\n",
            "Average training loss for epoch 11: 0.26703\n",
            "Average validation loss for epoch 11: 0.26187\n",
            "Average training loss for epoch 12: 0.25647\n",
            "Average validation loss for epoch 12: 0.23713\n",
            "Average training loss for epoch 13: 0.25582\n",
            "Average validation loss for epoch 13: 0.23258\n",
            "Average training loss for epoch 14: 0.24591\n",
            "Average validation loss for epoch 14: 0.22912\n",
            "Average training loss for epoch 15: 0.24436\n",
            "Average validation loss for epoch 15: 0.23198\n",
            "Average training loss for epoch 16: 0.23128\n",
            "Average validation loss for epoch 16: 0.22795\n",
            "Average training loss for epoch 17: 0.23632\n",
            "Average validation loss for epoch 17: 0.22368\n",
            "Average training loss for epoch 18: 0.23575\n",
            "Average validation loss for epoch 18: 0.22920\n",
            "Average training loss for epoch 19: 0.22718\n",
            "Average validation loss for epoch 19: 0.23384\n",
            "Average training loss for epoch 20: 0.23129\n",
            "Average validation loss for epoch 20: 0.23614\n",
            "Average training loss for epoch 21: 0.22874\n",
            "Average validation loss for epoch 21: 0.22687\n",
            "Average training loss for epoch 22: 0.22108\n",
            "Average validation loss for epoch 22: 0.22338\n",
            "Average training loss for epoch 23: 0.22915\n",
            "Average validation loss for epoch 23: 0.23734\n",
            "Average training loss for epoch 24: 0.22778\n",
            "Average validation loss for epoch 24: 0.23843\n",
            "Average training loss for epoch 25: 0.21635\n",
            "Average validation loss for epoch 25: 0.22995\n",
            "Average training loss for epoch 26: 0.21671\n",
            "Average validation loss for epoch 26: 0.23084\n",
            "Average training loss for epoch 27: 0.21102\n",
            "Average validation loss for epoch 27: 0.22447\n",
            "Average training loss for epoch 28: 0.20945\n",
            "Average validation loss for epoch 28: 0.22343\n",
            "Average training loss for epoch 29: 0.20825\n",
            "Average validation loss for epoch 29: 0.24152\n",
            "Average training loss for epoch 30: 0.20797\n",
            "Average validation loss for epoch 30: 0.22521\n",
            "Average training loss for epoch 1: 1.73418\n",
            "Average validation loss for epoch 1: 1.17900\n",
            "Average training loss for epoch 2: 0.88473\n",
            "Average validation loss for epoch 2: 0.82391\n",
            "Average training loss for epoch 3: 0.63505\n",
            "Average validation loss for epoch 3: 0.56582\n",
            "Average training loss for epoch 4: 0.49962\n",
            "Average validation loss for epoch 4: 0.48691\n",
            "Average training loss for epoch 5: 0.43200\n",
            "Average validation loss for epoch 5: 0.43531\n",
            "Average training loss for epoch 6: 0.39545\n",
            "Average validation loss for epoch 6: 0.42474\n",
            "Average training loss for epoch 7: 0.41967\n",
            "Average validation loss for epoch 7: 0.40215\n",
            "Average training loss for epoch 8: 0.36381\n",
            "Average validation loss for epoch 8: 0.36656\n",
            "Average training loss for epoch 9: 0.33020\n",
            "Average validation loss for epoch 9: 0.33362\n",
            "Average training loss for epoch 10: 0.29721\n",
            "Average validation loss for epoch 10: 0.30590\n",
            "Average training loss for epoch 11: 0.28331\n",
            "Average validation loss for epoch 11: 0.29635\n",
            "Average training loss for epoch 12: 0.26910\n",
            "Average validation loss for epoch 12: 0.28329\n",
            "Average training loss for epoch 13: 0.27063\n",
            "Average validation loss for epoch 13: 0.27808\n",
            "Average training loss for epoch 14: 0.25774\n",
            "Average validation loss for epoch 14: 0.27315\n",
            "Average training loss for epoch 15: 0.25075\n",
            "Average validation loss for epoch 15: 0.26904\n",
            "Average training loss for epoch 16: 0.24605\n",
            "Average validation loss for epoch 16: 0.25801\n",
            "Average training loss for epoch 17: 0.24561\n",
            "Average validation loss for epoch 17: 0.26720\n",
            "Average training loss for epoch 18: 0.23900\n",
            "Average validation loss for epoch 18: 0.27618\n",
            "Average training loss for epoch 19: 0.24075\n",
            "Average validation loss for epoch 19: 0.26657\n",
            "Average training loss for epoch 20: 0.23783\n",
            "Average validation loss for epoch 20: 0.25540\n",
            "Average training loss for epoch 21: 0.23263\n",
            "Average validation loss for epoch 21: 0.25772\n",
            "Average training loss for epoch 22: 0.23257\n",
            "Average validation loss for epoch 22: 0.25480\n",
            "Average training loss for epoch 23: 0.22626\n",
            "Average validation loss for epoch 23: 0.25506\n",
            "Average training loss for epoch 24: 0.22663\n",
            "Average validation loss for epoch 24: 0.25689\n",
            "Average training loss for epoch 25: 0.22505\n",
            "Average validation loss for epoch 25: 0.24680\n",
            "Average training loss for epoch 26: 0.22077\n",
            "Average validation loss for epoch 26: 0.25992\n",
            "Average training loss for epoch 27: 0.22041\n",
            "Average validation loss for epoch 27: 0.25541\n",
            "Average training loss for epoch 28: 0.21574\n",
            "Average validation loss for epoch 28: 0.25073\n",
            "Average training loss for epoch 29: 0.21493\n",
            "Average validation loss for epoch 29: 0.25004\n",
            "Average training loss for epoch 30: 0.21088\n",
            "Average validation loss for epoch 30: 0.25971\n",
            "Average training loss for epoch 1: 1.27282\n",
            "Average validation loss for epoch 1: 0.70278\n",
            "Average training loss for epoch 2: 0.53845\n",
            "Average validation loss for epoch 2: 0.49966\n",
            "Average training loss for epoch 3: 0.41472\n",
            "Average validation loss for epoch 3: 0.41119\n",
            "Average training loss for epoch 4: 0.34493\n",
            "Average validation loss for epoch 4: 0.34339\n",
            "Average training loss for epoch 5: 0.29873\n",
            "Average validation loss for epoch 5: 0.32761\n",
            "Average training loss for epoch 6: 0.27750\n",
            "Average validation loss for epoch 6: 0.29976\n",
            "Average training loss for epoch 7: 0.25910\n",
            "Average validation loss for epoch 7: 0.27087\n",
            "Average training loss for epoch 8: 0.25249\n",
            "Average validation loss for epoch 8: 0.29500\n",
            "Average training loss for epoch 9: 0.25146\n",
            "Average validation loss for epoch 9: 0.27486\n",
            "Average training loss for epoch 10: 0.24035\n",
            "Average validation loss for epoch 10: 0.28685\n",
            "Average training loss for epoch 11: 0.23800\n",
            "Average validation loss for epoch 11: 0.27340\n",
            "Average training loss for epoch 12: 0.23074\n",
            "Average validation loss for epoch 12: 0.28376\n",
            "Average training loss for epoch 13: 0.22784\n",
            "Average validation loss for epoch 13: 0.26325\n",
            "Average training loss for epoch 14: 0.22408\n",
            "Average validation loss for epoch 14: 0.26412\n",
            "Average training loss for epoch 15: 0.21994\n",
            "Average validation loss for epoch 15: 0.26252\n",
            "Average training loss for epoch 16: 0.21823\n",
            "Average validation loss for epoch 16: 0.25924\n",
            "Average training loss for epoch 17: 0.21676\n",
            "Average validation loss for epoch 17: 0.27534\n",
            "Average training loss for epoch 18: 0.20966\n",
            "Average validation loss for epoch 18: 0.26108\n",
            "Average training loss for epoch 19: 0.21673\n",
            "Average validation loss for epoch 19: 0.26680\n",
            "Average training loss for epoch 20: 0.21006\n",
            "Average validation loss for epoch 20: 0.27009\n",
            "Average training loss for epoch 21: 0.20615\n",
            "Average validation loss for epoch 21: 0.27331\n",
            "Average training loss for epoch 22: 0.20654\n",
            "Average validation loss for epoch 22: 0.27119\n",
            "Average training loss for epoch 23: 0.20186\n",
            "Average validation loss for epoch 23: 0.25238\n",
            "Average training loss for epoch 24: 0.19785\n",
            "Average validation loss for epoch 24: 0.26371\n",
            "Average training loss for epoch 25: 0.19492\n",
            "Average validation loss for epoch 25: 0.26423\n",
            "Average training loss for epoch 26: 0.19175\n",
            "Average validation loss for epoch 26: 0.26890\n",
            "Average training loss for epoch 27: 0.19049\n",
            "Average validation loss for epoch 27: 0.26055\n",
            "Average training loss for epoch 28: 0.18966\n",
            "Average validation loss for epoch 28: 0.27192\n",
            "Average training loss for epoch 29: 0.18783\n",
            "Average validation loss for epoch 29: 0.26689\n",
            "Average training loss for epoch 30: 0.18764\n",
            "Average validation loss for epoch 30: 0.25370\n",
            "Average training loss for epoch 1: 1.86960\n",
            "Average validation loss for epoch 1: 1.46082\n",
            "Average training loss for epoch 2: 0.95191\n",
            "Average validation loss for epoch 2: 0.66075\n",
            "Average training loss for epoch 3: 0.63427\n",
            "Average validation loss for epoch 3: 0.54598\n",
            "Average training loss for epoch 4: 0.52997\n",
            "Average validation loss for epoch 4: 0.49016\n",
            "Average training loss for epoch 5: 0.43132\n",
            "Average validation loss for epoch 5: 0.36748\n",
            "Average training loss for epoch 6: 0.35104\n",
            "Average validation loss for epoch 6: 0.33630\n",
            "Average training loss for epoch 7: 0.31979\n",
            "Average validation loss for epoch 7: 0.31759\n",
            "Average training loss for epoch 8: 0.30493\n",
            "Average validation loss for epoch 8: 0.31611\n",
            "Average training loss for epoch 9: 0.29208\n",
            "Average validation loss for epoch 9: 0.30759\n",
            "Average training loss for epoch 10: 0.27880\n",
            "Average validation loss for epoch 10: 0.29176\n",
            "Average training loss for epoch 11: 0.26672\n",
            "Average validation loss for epoch 11: 0.31862\n",
            "Average training loss for epoch 12: 0.26538\n",
            "Average validation loss for epoch 12: 0.29611\n",
            "Average training loss for epoch 13: 0.26028\n",
            "Average validation loss for epoch 13: 0.33364\n",
            "Average training loss for epoch 14: 0.24929\n",
            "Average validation loss for epoch 14: 0.31282\n",
            "Average training loss for epoch 15: 0.25776\n",
            "Average validation loss for epoch 15: 0.32710\n",
            "Average training loss for epoch 16: 0.24997\n",
            "Average validation loss for epoch 16: 0.29633\n",
            "Average training loss for epoch 17: 0.24516\n",
            "Average validation loss for epoch 17: 0.31109\n",
            "Average training loss for epoch 18: 0.24083\n",
            "Average validation loss for epoch 18: 0.28851\n",
            "Average training loss for epoch 19: 0.24458\n",
            "Average validation loss for epoch 19: 0.30220\n",
            "Average training loss for epoch 20: 0.24466\n",
            "Average validation loss for epoch 20: 0.29617\n",
            "Average training loss for epoch 21: 0.23920\n",
            "Average validation loss for epoch 21: 0.29007\n",
            "Average training loss for epoch 22: 0.23373\n",
            "Average validation loss for epoch 22: 0.28890\n",
            "Average training loss for epoch 23: 0.23178\n",
            "Average validation loss for epoch 23: 0.28740\n",
            "Average training loss for epoch 24: 0.22835\n",
            "Average validation loss for epoch 24: 0.28828\n",
            "Average training loss for epoch 25: 0.23091\n",
            "Average validation loss for epoch 25: 0.27790\n",
            "Average training loss for epoch 26: 0.22533\n",
            "Average validation loss for epoch 26: 0.30368\n",
            "Average training loss for epoch 27: 0.22185\n",
            "Average validation loss for epoch 27: 0.30493\n",
            "Average training loss for epoch 28: 0.21783\n",
            "Average validation loss for epoch 28: 0.30380\n",
            "Average training loss for epoch 29: 0.22034\n",
            "Average validation loss for epoch 29: 0.29460\n",
            "Average training loss for epoch 30: 0.22604\n",
            "Average validation loss for epoch 30: 0.29967\n",
            "Average training loss for epoch 1: 1.96484\n",
            "Average validation loss for epoch 1: 1.26889\n",
            "Average training loss for epoch 2: 1.06215\n",
            "Average validation loss for epoch 2: 0.88946\n",
            "Average training loss for epoch 3: 0.74179\n",
            "Average validation loss for epoch 3: 0.65968\n",
            "Average training loss for epoch 4: 0.58390\n",
            "Average validation loss for epoch 4: 0.55551\n",
            "Average training loss for epoch 5: 0.56851\n",
            "Average validation loss for epoch 5: 0.52882\n",
            "Average training loss for epoch 6: 0.51811\n",
            "Average validation loss for epoch 6: 0.50710\n",
            "Average training loss for epoch 7: 0.44529\n",
            "Average validation loss for epoch 7: 0.42346\n",
            "Average training loss for epoch 8: 0.33231\n",
            "Average validation loss for epoch 8: 0.35301\n",
            "Average training loss for epoch 9: 0.30369\n",
            "Average validation loss for epoch 9: 0.32665\n",
            "Average training loss for epoch 10: 0.29007\n",
            "Average validation loss for epoch 10: 0.57029\n",
            "Average training loss for epoch 11: 0.29228\n",
            "Average validation loss for epoch 11: 0.34117\n",
            "Average training loss for epoch 12: 0.28580\n",
            "Average validation loss for epoch 12: 0.30970\n",
            "Average training loss for epoch 13: 0.27579\n",
            "Average validation loss for epoch 13: 0.30899\n",
            "Average training loss for epoch 14: 0.27031\n",
            "Average validation loss for epoch 14: 0.52354\n",
            "Average training loss for epoch 15: 0.25279\n",
            "Average validation loss for epoch 15: 0.30732\n",
            "Average training loss for epoch 16: 0.24892\n",
            "Average validation loss for epoch 16: 0.29256\n",
            "Average training loss for epoch 17: 0.25348\n",
            "Average validation loss for epoch 17: 0.30026\n",
            "Average training loss for epoch 18: 0.24649\n",
            "Average validation loss for epoch 18: 0.30544\n",
            "Average training loss for epoch 19: 0.24334\n",
            "Average validation loss for epoch 19: 0.30087\n",
            "Average training loss for epoch 20: 0.23316\n",
            "Average validation loss for epoch 20: 0.28185\n",
            "Average training loss for epoch 21: 0.23559\n",
            "Average validation loss for epoch 21: 0.27693\n",
            "Average training loss for epoch 22: 0.23503\n",
            "Average validation loss for epoch 22: 0.30267\n",
            "Average training loss for epoch 23: 0.22648\n",
            "Average validation loss for epoch 23: 0.29389\n",
            "Average training loss for epoch 24: 0.23897\n",
            "Average validation loss for epoch 24: 0.28975\n",
            "Average training loss for epoch 25: 0.22262\n",
            "Average validation loss for epoch 25: 0.27871\n",
            "Average training loss for epoch 26: 0.22754\n",
            "Average validation loss for epoch 26: 0.27690\n",
            "Average training loss for epoch 27: 0.24487\n",
            "Average validation loss for epoch 27: 0.29816\n",
            "Average training loss for epoch 28: 0.22344\n",
            "Average validation loss for epoch 28: 0.30108\n",
            "Average training loss for epoch 29: 0.21939\n",
            "Average validation loss for epoch 29: 0.29923\n",
            "Average training loss for epoch 30: 0.21706\n",
            "Average validation loss for epoch 30: 0.29358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MEAN RMSE FOR X: {} \\nSTD OF RMSE FOR X: {}\".format(*get_rmse_metric(nn_preds,data['target'].values)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjbV4ansiFcs",
        "outputId": "73258f6a-5d49-4c54-f196-f6f7d7ff471d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEAN RMSE FOR X: 0.2447558256453773 \n",
            "STD OF RMSE FOR X: 0.015936914275448147\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}